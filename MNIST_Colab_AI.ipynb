{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiMvCIXSU7Kt",
        "outputId": "73e0a24c-6c74-468d-80d5-ce90e9930992"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
            "train_images shape: (60000, 28, 28)\n",
            "train_labels shape: (60000,)\n",
            "test_images shape: (10000, 28, 28)\n",
            "test_labels shape: (10000,)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim  # Provides optimization algorithms like Adam, SGD, etc.\n",
        "from torch.utils.data import DataLoader, TensorDataset  # Utilities for handling datasets and batching.\n",
        "from tensorflow import keras\n",
        "\n",
        "# Download and load the MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Print shapes to confirm\n",
        "print(\"train_images shape:\", X_train.shape)\n",
        "print(\"train_labels shape:\", y_train.shape)\n",
        "print(\"test_images shape:\", X_test.shape)\n",
        "print(\"test_labels shape:\", y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Section.\n"
      ],
      "metadata": {
        "id": "ZCFrpfnlrqMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Convert the data into PyTorch tensors and normalize the pixel values.\n",
        "# Tensors are PyTorch's data structures, similar to multi-dimensional arrays.\n",
        "# Normalization scales the pixel values (originally between 0 and 255) to a range of 0 to 1, which helps the model train more effectively.\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1) / 255.0  # Add a channel dimension for grayscale images.\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "\n",
        "# Similarly, normalize the test data.\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32).unsqueeze(1) / 255.0\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Use DataLoader to create batches of data for training.\n",
        "# Batching divides the dataset into smaller groups (batches) to process during training, which reduces memory usage and speeds up computation.\n",
        "batch_size = 64  # Number of samples per batch.\n",
        "train_dataset = TensorDataset(X_train, y_train)  # Combine the training data and labels into a dataset.\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  # Shuffle the data to improve training.\n",
        "\n",
        "# Define the CNN model.\n",
        "# A CNN is a type of neural network designed to process image data by extracting spatial features.\n",
        "cnn_model = nn.Sequential(\n",
        "    # First convolutional layer:\n",
        "    # - Input: 1 channel (grayscale image).\n",
        "    # - Output: 16 feature maps.\n",
        "    # - Kernel size: 3x3 (small sliding window to detect patterns).\n",
        "    # - Stride: 1 (move the kernel one pixel at a time).\n",
        "    # - Padding: 1 (add a border of zeros to preserve image dimensions).\n",
        "    nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
        "    nn.ReLU(),  # Activation function: Introduces non-linearity to the model.\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2),  # Max pooling: Reduces the spatial dimensions by half.\n",
        "\n",
        "    # Second convolutional layer:\n",
        "    # - Input: 16 feature maps.\n",
        "    # - Output: 32 feature maps.\n",
        "    nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "    nn.ReLU(),  # Activation function.\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2),  # Further reduces spatial dimensions.\n",
        "\n",
        "    # Flatten layer:\n",
        "    # Converts the 2D feature maps into a 1D vector to feed into fully connected layers.\n",
        "    nn.Flatten(),\n",
        "\n",
        "    # Fully connected layer:\n",
        "    # - Input: Flattened vector (32 * 7 * 7).\n",
        "    # - Output: 64 hidden units.\n",
        "    nn.Linear(32 * 7 * 7, 64),\n",
        "    nn.ReLU(),  # Activation function.\n",
        "\n",
        "    # Output layer:\n",
        "    # - Input: 64 hidden units.\n",
        "    # - Output: 10 classes (digits 0-9).\n",
        "    nn.Linear(64, 10)\n",
        ")\n",
        "\n",
        "# Check if a GPU is available and move the model to the GPU for faster computation.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "cnn_model.to(device)\n",
        "\n",
        "# Define the loss function and optimizer.\n",
        "# - Loss function: Measures the difference between the predicted and actual labels. We use CrossEntropyLoss for classification tasks.\n",
        "# - Optimizer: Updates the model's parameters to minimize the loss. We use the Adam optimizer with a learning rate of 0.001.\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop: Train the model for a specified number of epochs (iterations over the entire dataset).\n",
        "epochs = 10  # Number of times the model sees the entire dataset.\n",
        "for epoch in range(epochs):\n",
        "    cnn_model.train()  # Set the model to training mode (enables features like dropout, if used).\n",
        "    epoch_loss = 0  # Initialize the total loss for the epoch.\n",
        "    for batch_x, batch_y in train_loader:  # Iterate over batches of data.\n",
        "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)  # Move data to the GPU (if available).\n",
        "        optimizer.zero_grad()  # Reset gradients from the previous step.\n",
        "        outputs = cnn_model(batch_x)  # Perform a forward pass (predict outputs).\n",
        "        loss = loss_fn(outputs, batch_y)  # Compute the loss for the batch.\n",
        "        loss.backward()  # Perform a backward pass (compute gradients).\n",
        "        optimizer.step()  # Update the model's parameters using the optimizer.\n",
        "        epoch_loss += loss.item()  # Accumulate the loss for the epoch.\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / len(train_loader):.4f}\")  # Print the average loss for the epoch.\n",
        "\n",
        "# Save the trained model's parameters to a file for later use.\n",
        "torch.save(cnn_model.state_dict(), \"cnn_model.pth\")\n",
        "\n",
        "# Testing the model: Evaluate its performance on the test dataset.\n",
        "cnn_model.eval()  # Set the model to evaluation mode (disables features like dropout).\n",
        "X_test, y_test = X_test.to(device), y_test.to(device)  # Move test data to the GPU (if available).\n",
        "with torch.no_grad():  # Disable gradient computation (not needed during testing).\n",
        "    outputs = cnn_model(X_test)  # Perform a forward pass on the test data.\n",
        "    _, predicted = torch.max(outputs, 1)  # Get the predicted class for each image.\n",
        "    accuracy = (predicted == y_test).sum().item() / y_test.size(0)  # Calculate the accuracy.\n",
        "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")  # Print the accuracy as a percentage.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X26TVRQOplvA",
        "outputId": "fd977041-d20f-419f-ae55-46890586eb6d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.2441\n",
            "Epoch 2/10, Loss: 0.0745\n",
            "Epoch 3/10, Loss: 0.0534\n",
            "Epoch 4/10, Loss: 0.0418\n",
            "Epoch 5/10, Loss: 0.0345\n",
            "Epoch 6/10, Loss: 0.0280\n",
            "Epoch 7/10, Loss: 0.0239\n",
            "Epoch 8/10, Loss: 0.0197\n",
            "Epoch 9/10, Loss: 0.0179\n",
            "Epoch 10/10, Loss: 0.0141\n",
            "Test Accuracy: 98.91%\n"
          ]
        }
      ]
    }
  ]
}